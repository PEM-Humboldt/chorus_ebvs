# Chorus Essential Biodiversity Variables
This repository contains Python scripts that allow creating a dataset of Essential Biodiversity Variables (EBV-ready dataset) for the class of species traits by combining two sources of data:

* Files generated by inference models of acoustic activity of anuran amphibians.
* Files of climatic variables (dataloggers and nearby weather stations). These data are read and harmonized according to the chosen locations and temporal range, and the result is saved in a NetCDF file.

## Requiriments

Python 3.10

```
matplotlib==3.6.2
netcdf4==1.6.2
numpy==1.22.4
openpyxl==3.0.10
pandas==1.5.2
pillow==9.3.0
scikit-learn==1.0.2
seaborn==0.12.2
xarray==2023.3.0
```

## How to run

1. Install [Conda](https://docs.conda.io/projects/conda/en/stable/)

2. Clone this repository

```
git clone https://github.com/breyner-posso/chorus_ebvs/
```

3. Unzip the content of the folder 'sample_data'. Once unzipped, the directory structure should appear as follows:

```
sample_data
├── INCT2055
│   ├── datalogger
|       └── INCT20955_datalogger_20191220_20200429.xlsx
│   ├── model_predictions
│       └── INCT20955_inferences.csv
│   └── weather_station
│       └── INCT20955_wstation_A845_20191220_20200419.xlsx
└── metadata_workflow.xlsx
```

4. Create environment and install requirements

```
cd chorus_ebvs
conda create -n chorus_env python=3.10 -y
conda activate chorus_env
conda install --file requirements.txt
```

5. Before executing the scripts, define the following:
+ location
+ date range
+ path to the folder containing the data
+ nane of the file that contains the metadata of the locations
+ name of the file that contains the metadata for the EBV-ready dataset that is going to be created
+ name of the NetCDF file that will contain the EBV-ready dataset. Be sure to include the .nc extension in the file name
```
# Enter location id
location_id = 'INCT20955'
# Enter dates as string in YYYY-MM-DD format
start_date = "2020-01-01"
end_date = "2020-01-31"
# Enter the path to the folder containing the files
folder_path = "sample_data"
# Enter the nane of the file that contains the metadata of the locations
locations_metadata_file = "locations_metadata.xlsx"
# Enter the name of the file that contains the metadata for the EBV-ready dataset that is going to be created
ebvs_metadata_file = "ebvs_metadata.xlsx"
# Enter the name of the NetCDF file that will contain the EBV-ready dataset. Be sure to include the .nc extension in the file name
ebvs_file_name = "INCT20955_phenology.nc"
```

8. Open the primary observations files, then load them into a pandas dataframe

```
import chorus_get_data as gdata

# get_inference() imports inference raw data
df_inf = gdata.get_inference(folder_path, location_id, start_date, end_date)

# get_datalogger() imports weather information from dataloggers
df_dlog = gdata.get_datalogger(folder_path, location_id, start_date, end_date)

# get_wstation() imports weather information from weather station
df_wst = gdata.get_wstation(folder_path, location_id, start_date, end_date)

# get_metadata() imports metadata
df_meta = gdata.get_metadata(folder_path, locations_metadata_file, location_id)
```

9. Harmonize data

```
import chorus_harmonize_data as hdata

df_inf_h,df_dlog_h,df_wst_h = hdata.harmonize3(df_inf,df_dlog,df_wst)
```

10. Create the EBV-ready dataset

